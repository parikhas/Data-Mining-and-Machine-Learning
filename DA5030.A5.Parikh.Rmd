---
title: "R Notebook"
output:
  word_document: default
  pdf_document: default
  html_notebook: default
---
Problem 1 (25 Pts)

Build an R Notebook of the bank loan decision tree example in the textbook on pages 136 to 149. Show each step and add appropriate documentation. Note that the provided dataset uses values 1 and 2 in default column whereas the book has no and yes in the default column. To fix any problems replace "no" with "1" and "yes" with "2" in the code that for matrix_dimensions. Alternatively, change the line
error_cost <- matrix(c(0, 1, 4, 0), nrow = 2, dimnames = matrix_dimensions) to error_cost <- matrix(c(0, 1, 4, 0), nrow = 2)

```{r}
# Load the dataset and install and load the required packages
#setwd('~/Documents/ML/')
#install.packages("RWeka")
library(RWeka)
#install.packages("C50")
library(C50)
library(gmodels)

credit <- read.csv('credit.csv')
```

```{r}
str(credit)
# Look at the table() output for some features
table(credit$checking_balance)
table(credit$savings_balance)
table(credit$default)
# Get summary of some numeric features
summary(credit$months_loan_duration)
summary(credit$amount)
```
Data preparation – creating random training and
test datasets

```{r}
# Choose 900 random values for the training set
set.seed(123)
train_sample <- sample(1000, 900)
str(train_sample)
# Select the rows in the train_sample vector to be in the training same and the others to be in the testing sample
credit_train <- credit[train_sample, ]
credit_test <- credit[-train_sample, ]
prop.table(table(credit_train$default))
prop.table(table(credit_test$default))
```
Step 3 – training a model on the data

```{r}
# # Exclude the 17th column which is the default class variable from the training data frame but supply it as the target factor vector for classification
credit_train$default <- as.factor(credit_train$default) # since C5.0 models require a factor outcome
credit_model <- C5.0(credit_train[-17], credit_train$default)
credit_model
summary(credit_model)
```
Step 4 – evaluating model performance

```{r}
#Apply decision tree to the test dataset
credit_pred <- predict(credit_model, credit_test)
#Compare predictions to the actual class values using the CrossTable() function in the gmodels package
CrossTable(credit_test$default, credit_pred,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual default', 'predicted default'))
```
Step 5 – improving model performance

Boosting the accuracy of decision trees

```{r}
# Add boosting to the decision tree by adding trials parameter
# Trials parameter indicates the number of separate decision trees to use in the boosted team
# Start with 10 trials which is the de facto standard

credit_boost10 <- C5.0(credit_train[-17], credit_train$default,
trials = 10)
credit_boost10
summary(credit_boost10)
# Use it on the test data
credit_boost_pred10 <- predict(credit_boost10, credit_test)
CrossTable(credit_test$default, credit_boost_pred10,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual default', 'predicted default'))
```
Assign a penalty to different types of errors, in order
to discourage a tree from making more costly mistakes. The penalties are designated in a cost matrix, which specifies how much costlier each error is, relative to any other prediction.

```{r}
matrix_dimensions <- list(c(1, 2), c(1, 2))
names(matrix_dimensions) <- c("predicted", "actual")
# Examine the net object
matrix_dimensions

# Define penalty values
error_cost <- matrix(c(0, 1, 4, 0), nrow = 2,
dimnames = matrix_dimensions)
error_cost
# Apply this in the decision tree
credit_cost <- C5.0(credit_train[-17], credit_train$default,
costs = error_cost)
credit_cost_pred <- predict(credit_cost, credit_test)
CrossTable(credit_test$default, credit_cost_pred,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual default', 'predicted default'))
```

Problem 2 (25 Pts)

Build and R Notebook of the poisonous mushrooms example using rule learners in the textbook on pages 160 to 168. Show each step and add appropriate documentation.

```{r}
# Load the dataset
mushrooms <- read.csv("mushrooms.csv", stringsAsFactors = T)
str(mushrooms)
# Delete veil_type variable since it does not vary across the samples and hence doen't provide any useful information
mushrooms$veil_type <- NULL
table(mushrooms$type)
```

Train a model on the data

```{r}
# Using the type ~ . formula, allow our first OneR() rule learner to consider all the possible features in the mushroom data while constructing its rules to predict type:
mushrooms_1R <- OneR(type ~ ., data = mushrooms)
mushrooms_1R
```
Evaluate model performance

```{r}
summary(mushrooms_1R)
```

Improve model performance

```{r}
# Train the JRip() rule learner allowing it to choose rules from all the available features:
mushroom_JRip <- JRip(type ~ ., data = mushrooms)
# Examine the rules
mushroom_JRip
```
Problem 3 (25 Pts)

So far we have explored four different approaches to classification: kNN, Naive Bayes, C5.0 Decision Trees, and RIPPER Rules. Comment on the differences of the algorithms and when each is generally used. Provide examples of when they work well and when they do not work well. Add your comments to your R Notebook. Be specific and explicit; however, no code examples are needed.

Answer:

kNN is a lazy learning algorithm since it carries out the classification by storing the data in memory and comparing new data to the stored one. There is no abstraction, just information retention. It used for classification when the features in the dataset are numeric since it uses distance measures to find the nearest neighbours. It will not work well when the features are nominal. 
For example it will work very well on a dataset classifying size based on measurements of height, waist, hips etc. 
But it will not work for classifying an email as spam or ham since it does not work well for categorical features.

Naive Bayes is a generative probabilistic model which is used when the features are categorical so that their occurence can be counted in the frequency table. It is an eager learning algorithm since it constructs a classification model based on the training data before receiving the new data for classification. FOr Naive Bayes, if the features are numeric, they have to be converted to nominal features by binning them. However, it is not well suited for data sets with numeric features. Naive Bayes will not work well when there are a lot of features and some features have weak effects since all the features are used with an equal weight in Naive Bayes. 
Naive Bayes is very commonly used for text classification(spam versus ham) and it works very well for the same.  
It will not work for small datasets since it needs a big data set in order to make reliable estimations of the probability of each class. 

C5.0 Decision Tress are a hiererchial tree structure where each node specifies a test of some feature, each branch corresponds to a classification and each leaf node provides a classification for the inteance. It is an eager learning algorith since it induces an abstract model from data and then carries out the classification by applying it to the new data. It works well for both numeric and categorical data. However it will not work if the features have a lot of levels(factors) since for each feature, there will have to be as many nodes as the number of factors. 
Decision trees work very well when the features are categorical and each feature doesn't have too many factors, for eg whether to go a particular restuarant based on the type of food, wait time, cost etc. It will not work very well if there are like 20 options for the type of food since it will have to make 20 nodes for that particular feature.

RIPPER(Repeated Incremental Pruning to Produce Error Reduction) is a rule based classification algorithm. It is derived from the Incremental Reduced Error Pruning(IREP) algorithm. It starts from an empty rule, then adds conditions till they improve the information gain and until it perfectly classifies a subset for splitting. It then prunes the rule and repeats the steps till it reaches a stopping criterion. It will work well on the task of classifying an animal. It will identify rules such as does it walk on land and have a tail, does it have fur etc. It will not work well if the data has all numeric features and they cannot be properly programmed into rules due to high variability.

References:
https://www.slideshare.net/marinasantini1/lecture-8-39706324
http://staffwww.itn.liu.se/~aidvi/courses/06/dm/lectures/lec4.pdf


Problem 4 (25 Pts)

Much of our focus so far has been on building a single model that is most accurate. In practice, data scientists often construct multiple models and then combine them into a single prediction model. This is referred to as a model ensemble. Two common techniques for assembling such models are boosting and bagging. Do some research and define what model ensembles are, why they are important, and how boosting and bagging function in the construction of assemble models. Be detailed and provide references to your research. You can use this excerpt from Kelleher, MacNamee, and D'Arcy, Fundamentals of Machine Learning for Predictive Data Analytics as a starting point. This book is an excellent resource for those who want to dig deeper into data mining and machine learning.

Answer: 

Model ensembles are algorithms which combine several machine learning models into a single optimal prediction model. They are used in order to decrease variance, biad and improve predictions. They are important since they improve the stability and accuracy of the model. The two most common techniques for assembling the models are boosting and bagging.

In bagging, each of the models selected for the ensemble is trained on a random sample of the original dataset. The size of the sample is the same as that of the dataset. Bootstrapping is used for making the random samples i.e. random sampling with replacement is carried out to make up the samples. The samples made are called bootstrap samples. 
Each of the bootstrap samples are used with one particular model. Sampling with replacement results in duplicates within each of the samples. Also each samples will be missing some of the dataset cases and hence each sample will be different. Consequently, each of the models trained will also be different. 
Once all the models used in the ensemble are trained, they are used with the new cases to make predictions for them. After all the predictiond have been made, either the most common prediction or the median prediction is selected for the classification. For continuous features, median is preferred over the mean since mean more affected by outliers. 

In boosting, each of the new models added to the emsemble is trained to give more attention for classifying the cases where the previous model had given wrong clssifications. This is done by adapting the dataset each time after a model has been trained in order to improve the accuracy of the next model using it by associating weights to each of the cases in the dataset. 
Initially all the instances have an equal weight which is 1/n where n = number of cases. The weights are used as a distribution over which the sampling of dataset takes place for creating a replicated training dataset. In the replicated dataset, ecah of the cases is replicated proportionally to its weight. 
Once the first model has been trained and we know the correct and the incorrect classifications, the total error E is calculated by adding the weights of the cases with wrong predictions.Then the weights are adjusted such that the weights of the incorrect classifications are increased (w[i] = w[i]*(1/2*E)) and the weights of the correct classifications are decreased(w[i] = w[i]*loge(1-E)/1). It also calculates a confidence factor such that it increases as the error decreases. 
Once the models have been created, the ensemble makes predictions using a weighted aggregate of the predictions made by each of the models. For classifying categorical features, the ensemble gives the majority level using a weighted vote while for numeric features, it gives the weighted mean.

Refernces:
https://da5030.weebly.com/uploads/8/6/5/9/8659576/baggingboostingkelleher.pdf
https://www.analyticsvidhya.com/blog/2015/08/introduction-ensemble-learning/

























