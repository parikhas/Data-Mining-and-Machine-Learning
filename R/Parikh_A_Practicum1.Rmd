---
title: "R Notebook"
output:
  word_document: default
  pdf_document: default
  html_notebook: default
---


```{r}
setwd('~/Documents/ML')
library(dplyr)
library(class)
library(caret)
df <- read.csv('glass.data.txt', header = F)
colnames(df) <- c('ID',"RI",'Na',"Mg","Al","Si","K", "Ca", "Ba", "Fe", "Type")
```

3. (5 pts) Create a histogram of the Na column and overlay a normal curve; visually determine whether the data is normally distributed. You may use the code from this tutorial. Does the k-NN algorithm require normally distributed data or is it a non-parametric method? Comment on your findings. 

The data is normally distributed.
The k-NN algorithm does not require normally distributed data. It is a non-parametric method.
```{r}
x <- df$Na 
h<-hist(x, breaks=10, xlab="Na", 
  	main="Histogram with Normal Curve") 
xfit<-seq(min(x),max(x),length=40) 
yfit<-dnorm(xfit,mean=mean(x),sd=sd(x)) 
yfit <- yfit*diff(h$mids[1:2])*length(x) 
lines(xfit, yfit, lwd=2)
```
4. (5 pts) After removing the ID column (column 1), normalize the first two columns in the data set using min-max normalization.
```{r}
df_norm <- df[,-1] #Removind the ID column
# Normalixe using min-max normalization
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x))) }
df_n <- as.data.frame(lapply(df_norm[1:2], normalize))
head(df_n)
```

5. (5 pts) Normalize the remaining columns, except the last one, using z-score standardization. The last column is the glass type and so it is excluded.
```{r}
# Normalize using z score
z_score <- function(x){
  return((x - mean(x))/sd(x))
}
dd <- as.data.frame(lapply(df_norm[3:9], z_score))
data <- cbind(df_n,dd,Type = df_norm$Type)
```

6. (5 pts) The data set is sorted, so creating a validation data set requires random selection of elements. Create a stratified sample where you randomly select 50% of each of the cases for each glass type to be part of the validation data set. The remaining cases will form the training data set.
```{r}
# Check the nymber of cases for each glass type
data %>% group_by(Type) %>% summarise(rows = length(Type))
#install.packages("kimisc")
library(kimisc)
# Select half of the cases for each glass typr for the training data set
df.1 <- sample.rows(subset(data, Type == "1"), 35)
df.2 <- sample.rows(subset(data, Type == "2"), 38)
df.3 <- sample.rows(subset(data, Type == "3"), 9)
df.5 <- sample.rows(subset(data, Type == "5"), 6)
df.6 <- sample.rows(subset(data, Type == "6"), 5)
df.7 <- sample.rows(subset(data, Type == "7"), 14)

test_df <- rbind(df.1,df.2,df.3,df.5,df.6,df.7)
train_df <- anti_join(data, test_df)
```

7. (20 pts) :. Use the whole normalized data set for this; not just the training data set. Note that you need to normalize the values of the new cases the same way as you normalized the original data.
RI = 1.51621 | 12.53 | 3.48 | 1.39 | 73.39 | 0.60 | 8.55 | 0.00 | Fe = 0.05
RI = 1.5098 | 12.77 | 1.85 | 1.81 | 72.69 | 0.59 | 10.01 | 0.00 | Fe = 0.01

Implement the k-NN algorithm in R (do not use an implementation of k-NN from a package) and use your algorithm with a k=10 to predict the glass type for the following two cases

The glass type for the first unknown case is 1 and for the second case, it is 6
```{r}
unk1 <- c(1.51621,12.53,3.48,1.39,73.39,0.60,8.55,0.00,0.05)
unk2 <- c(1.5098,12.77,1.85,1.81,72.69,0.59,10.01,0.00,0.01)

# Normalizing the new cases
norm1 <- function(x) {
return ((x - 1.5112) / (1.5339 - 1.5112)) }
unk1[1] <- norm1(unk1[1])
unk2[1] <- norm1(unk2[1])
norm2 <- function(x) {
return ((x - 10.73) / (17.38 - 10.73)) }
unk1[2] <- norm2(unk1[2])
unk2[2] <- norm2(unk2[2])

unk1[3] <- (unk1[3] - 2.6845)/1.4424
unk1[4] <- (unk1[4] - 1.4449)/0.4993
unk1[5] <- (unk1[5] - 72.6509)/0.7745
unk1[6] <- (unk1[6] - 0.4971)/0.6522
unk1[7] <- (unk1[7] - 8.9570)/1.4232
unk1[8] <- (unk1[8] - 0.1750)/0.4972
unk1[9] <- (unk1[9] - 0.0570)/0.0974

unk2[3] <- (unk2[3] - 2.6845)/1.4424
unk2[4] <- (unk2[4] - 1.4449)/0.4993
unk2[5] <- (unk2[5] - 72.6509)/0.7745
unk2[6] <- (unk2[6] - 0.4971)/0.6522
unk2[7] <- (unk2[7] - 8.9570)/1.4232
unk2[8] <- (unk2[8] - 0.1750)/0.4972
unk2[9] <- (unk2[9] - 0.0570)/0.0974

unk1 <- as.data.frame(unk1)
unk1 <-t(unk1)
colnames(unk1) <- c("RI",'Na',"Mg","Al","Si","K", "Ca", "Ba", "Fe")
unk2 <- as.data.frame(unk2)
unk2 <- t(unk2)
colnames(unk1) <- c("RI",'Na',"Mg","Al","Si","K", "Ca", "Ba", "Fe")

# Calculating distances

dist <- function(p,q)
{
  d <- 0
  for (i in 1:length(p)) {
    d <- d + (p[i] - q[i])^2
  }
  dist <- sqrt(d)
}
neighbors <- function (data, unk)
{
   m <- nrow(data)
   ds <- numeric(m)
   #q <- as.numeric(unk)
   for (i in 1:m) {
     p <- data[i,c(1:9)]
   #  for (i in 1:nrow(unk))
      q <- unk[c(1:9)] #new
      ds[i] <- dist(p,q)
   }
   neighbors <- ds
}

k.closest <- function(neighbors,k)
{
  ordered.neighbors <- order(neighbors)
  k.closest <- ordered.neighbors[1:k]
}
#f <- k.closest(n,10)
Mode <- function(x) 
{
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}
#data$Type[f]
#Mode(data$Type[f])

knn_ <- function (train, u, k)
{
  nb <- neighbors(train,u)
  f <- k.closest(nb,k)
  knn_ <- Mode(train$Type[f])
}
nn1 <- knn_(data,unk1,10)
nn1

nn2 <- knn_(data,unk2,10)
nn2
```


8. (10 pts) Apply the knn function from the class package with k=14 and redo the cases from Question (7).
```{r}
library(class)
head(train_df)
head(test_df)
data_n <- data[,1:9]
data_labels <- data[1:214,10]
#train_labels <- train_df[1:107, 10]
#test_labels <- test_df[1:107, 10]
prc_test_pred1 <- knn(train = data_n, test = unk1, cl = data_labels, k=14)
prc_test_pred1
prc_test_pred2 <- knn(train = data_n, test = unk2, cl = data_labels, k=14)
prc_test_pred2
```

9. (10 pts) Determine the accuracy of the knn function with k=14 from the class package by applying it against each case in the validation data set. What is the percentage of correct classifications?

The percentage of correct classifications is 64.49%
```{r}
test_n <- test_df[,1:9]
test_labels <- test_df[1:107, 10]
test_pred <- knn(train = data_n, test = test_n, cl = data_labels, k=14)
library(gmodels)
CrossTable(x=test_labels, y=test_pred,prop.chisq = F)
prc_test_labels <- as.factor(test_labels)
confusionMatrix(test_pred, prc_test_labels)
```

10. (7 pts) Determine an optimal k by trying all values from 5 through 14 for your own k-NN algorithm implementation against the cases in the validation data set. What is the optimal k, i.e., the k that results in the best accuracy? Plot k versus accuracy.

k = 6 gives the highest accuracy
```{r}
train_n <- train_df[,1:9]
train_labels <- train_df[, 10]

#test_pred5 <- nrow(test_n)
test_pred5 <- seq(0,0,length.out=nrow(test_n))
test_pred6 <- seq(0,0,length.out=nrow(test_n))
test_pred7 <- seq(0,0,length.out=nrow(test_n))
test_pred8 <- seq(0,0,length.out=nrow(test_n))
test_pred9 <- seq(0,0,length.out=nrow(test_n))
test_pred10 <- seq(0,0,length.out=nrow(test_n))
test_pred11 <- seq(0,0,length.out=nrow(test_n))
test_pred12 <- seq(0,0,length.out=nrow(test_n))
test_pred13 <- seq(0,0,length.out=nrow(test_n))
test_pred14 <- seq(0,0,length.out=nrow(test_n))

for (i in 1:nrow(test_n)){
test_pred5[i] <- knn_(train_df, test_n[i,1:9], 5) 
}
test_pred5 <- as.factor(test_pred5)
CM <- confusionMatrix(test_pred5, prc_test_labels)
CM
# Accuracy : 0.6168  

for (i in 1:nrow(test_n)){
test_pred6[i] <- knn_(train_df, test_n[i,1:9], 6) 
}
test_pred6 <- as.factor(test_pred6)
confusionMatrix(test_pred6, prc_test_labels)
# Accuracy : 0.6262  

for (i in 1:nrow(test_n)){
test_pred7[i] <- knn_(train_df, test_n[i,1:9], 7) 
}
test_pred7 <- as.factor(test_pred7)
confusionMatrix(test_pred7, prc_test_labels)
# Accuracy : 0.6168  

for (i in 1:nrow(test_n)){
test_pred8[i] <- knn_(train_df, test_n[i,1:9], 8) 
}
test_pred8 <- as.factor(test_pred8)
confusionMatrix(test_pred8, prc_test_labels)
# Accuracy : 0.5607  

for (i in 1:nrow(test_n)){
test_pred9[i] <- knn_(train_df, test_n[i,1:9], 9) 
}
test_pred9 <- as.factor(test_pred9)
confusionMatrix(test_pred9, prc_test_labels)
# Accuracy : 0.5888 

for (i in 1:nrow(test_n)){
test_pred10[i] <- knn_(train_df, test_n[i,1:9], 10) 
}
test_pred10 <- as.factor(test_pred10)
confusionMatrix(test_pred10, prc_test_labels)
# Accuracy : 0.5981  

for (i in 1:nrow(test_n)){
test_pred11[i] <- knn_(train_df, test_n[i,1:9], 11) 
}
test_pred11 <- as.factor(test_pred11)
confusionMatrix(test_pred11, prc_test_labels)
# Accuracy : 0.5888   

for (i in 1:nrow(test_n)){
test_pred12[i] <- knn_(train_df, test_n[i,1:9], 12) 
}
test_pred12 <- as.factor(test_pred12)
confusionMatrix(test_pred12, prc_test_labels)
# Accuracy : 0.5701   

for (i in 1:nrow(test_n)){
test_pred13[i] <- knn_(train_df, test_n[i,1:9], 13) 
}
test_pred13 <- as.factor(test_pred13)
confusionMatrix(test_pred13, prc_test_labels)
# Accuracy : 0.6075  

for (i in 1:nrow(test_n)){
test_pred14[i] <- knn_(train_df, test_n[i,1:9], 14) 
}
test_pred14 <- as.factor(test_pred14)
confusionMatrix(test_pred14, prc_test_labels)
#Accuracy : 0.5888  

k <- c(5:14)
accuracy <- c(0.6168,0.6262,0.6168,0.5607,0.5888,0.5981,0.5888,0.5701,0.6075,0.5888)
# Run the next two lines of code together
plot(k,accuracy, xaxt="n")
axis(1, at = seq(5, 14, by = 1))
```

11. (5 pts) Create a plot of k (x-axis) versus error rate (percentage of incorrect classifications) using ggplot.
```{r}
library(ggplot2)
error <- rep(1, each=10)
error_rate <- error - accuracy
qplot(k,error_rate)
```

12. (5 pts) Produce a cross-table confusion matrix showing the accuracy of the classification using a package of your choice and a k of your choice.
```{r}
# Using package class and k =6
test_pred11 <- knn(train = train_n, test = test_n, cl = train_labels, k=6)
confusionMatrix(test_pred11, prc_test_labels)
# Accuracy is 0.6075
```

13. (3 pts) Comment on the run-time complexity of the k-NN for classifying w new cases using a training data set of n cases having m features. Assume that m is "large". How does this algorithm behave as w, n, and m increase? Would this algorithm be "fast" if the training data set and the number of features are large?

The run-time is positively correlated with the size of the training(n), the number of new cases(w) and the features(m).
If w, n and m increase, the run-time of the algorith will increase.
The algorith would not be "fast" if the training data set and the number of features are large.

2. 1. (10 pts) Investigate this data set of home prices in King County (USA). How many cases are there? How many features? Imagine you are a real estate broker and are advising home sellers on how much their home  is worth. Research and think about how you might use kNN to forecast (predict) the likely sales price for a home? Build a forecasting model with kNN and then forecast the price of some home (you can determine its features and you may build the algorithm yourself or use a package such as caret).

There are 21613 cases in this data set.
There are 21 features.
The features relevant to the house price seem to be bedrooms, bathrooms, sqft_living, sqft_loft, floors, condition, grade, sqft_above,yr_built, zipcode, lat and long
```{r}
hp <- read.csv('kc_house_data.csv')
summary(hp)
# Select the features relevant for predicting the house price
hpr <- subset(hp, , -c(id,date,waterfront,view,sqft_basement,yr_renovated,sqft_living15,sqft_lot15))
summary(hpr)

# Normalize the data and transform all the values to a common scale
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x))) }
hpr_n <- as.data.frame(lapply(hpr[2:13], normalize))
hpr_n <- cbind(price = hpr$price, hpr_n)

# Create train and test datasets
trainIndex <- createDataPartition(hpr_n$price, p = .8, 
                                  list = FALSE, times = 1)
hpTrain <- hpr_n[ trainIndex,]
hpTest  <- hpr_n[-trainIndex,]
 
set.seed(400)

ctrl <- trainControl(method = "cv", number = 10)
model_knn <- train(price ~., data = hpTrain, method = "knn", 
                       trControl = ctrl, 
                       tuneLength = 10)

model_knn

# Take a random row from the test data set to predict it's price
# Take all the features from 7th row except the price
unk1 <- hpTest[7,2:13]
unk1
# Here we are taking the normalized data for the prediction but if it is not, then it should be normalized like in the first problem

# Predict the house price using the knn model
predict_knn <- predict(model_knn, unk1)
predict_knn
# The predicted hourse price is 775400 dollars
act_house <- hpTest[7,]
act_house$price
```
2. How would you evaluate the model? While you need to only describe how to evaluate the model, you may calculate an actual metric

We can evaluate the model by using it with the testing data set and calculating an error metric such as Mean Absolute Deviation (MAD) or Mean Squared Error (MSE). The model is said to be a good one if the error metrics are low.

3. 1. (10 pts) Inspect the data set of occupancy rates for a series of time periods. Which forecasting method is most appropriate to use for forecast the next time period? Calculate a forecast for the next time period with a 95% prediction interval. Comment on the bias of your forecasting model.

There are no trends in this dataset and so a simple moving average seems the most appropriate forecasting method.
The range of the forecast for the next time period using 3- period simple moving average is from 22.42 to 40.91. 
The Mean Absolute Deviation is 5.89. Since the bias is not too strong, the moving average seems to be a suitable forecasting model.
```{r}
occ_rate <- read.csv('occupancyratestimeseries.csv')
plot(occ_rate$Period, occ_rate$OccupancyRate)

### Simple moving average (used 3-period)
n <- nrow(occ_rate)
last3 <- occ_rate[n:(n-2),2]
SMA <- mean(last3)
SMA

occ_rate$Ft <- 0
# Forecasting
for (i in 4:nrow(occ_rate)){
  occ_rate$Ft[i] <- (occ_rate$OccupancyRate[i-1]+ occ_rate$OccupancyRate[i-2]+occ_rate$OccupancyRate[i-3])/3
}
# Make a new column for Square error and initially put all values 0
occ_rate$E <- 0
# Calculating mean absolute deviaion for the forecasts
for (i in 4:nrow(occ_rate)){ # starting from 3rd row since there are no forecasts for the first 2 rows
  occ_rate$E[i] <- abs(occ_rate$OccupancyRate[i]-occ_rate$Ft[i])
}
# Taking mean using 163 observations since there are only 163 forecasts
MAD <- (sum(occ_rate$E))/163
MAD

# Forecast for the next time period with a 95% prediction interval
D <- 0.8*MAD
D
#CI = SMA +- (1.96*D)
CI1 <- SMA - (1.96*D)
CI1
CI2 <- SMA + (1.96*D)
CI2
```














