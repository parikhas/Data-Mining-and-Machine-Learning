---
title: "R Notebook"
output:
  pdf_document: default
  html_notebook: default
---

Problem 1

Build an R Notebook of the concrete strength example in the textbook on pages 232 to 239. Show each step and add appropriate documentation.
```{r}
#setwd('~/Documents/ML/')
library(caret)
library(dplyr)
library(neuralnet)
library(kernlab)
#install.packages("arules")
library(arules)
```

Step 1 – exploring and preparing the data

```{r}
concrete <- read.csv("concrete.csv")
str(concrete)
# Normalize the data
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
concrete_norm <- as.data.frame(lapply(concrete, normalize))
summary(concrete_norm$strength)
summary(concrete$strength)

# Partition the data into a training set with 75 percent of the examples and a testing set with 25 percent
concrete_train <- concrete_norm[1:773, ]
concrete_test <- concrete_norm[774:1030, ]
```

Step 2 – training a model on the data

```{r}

# We'll begin by training the simplest multilayer feedforward network with only a single hidden node:
concrete_model <- neuralnet(strength ~ cement + slag
+ ash + water + superplastic + coarseagg + fineagg + age,
data = concrete_train)
plot(concrete_model)
```
Step 3 – evaluating model performance

```{r}


# Generate predictions on the test dataset
model_results <- compute(concrete_model, concrete_test[1:8])
# Get the predicted values
predicted_strength <- model_results$net.result

# Find correlation between our predicted concrete strength and the true value
cor(predicted_strength, concrete_test$strength)
```

Step 4 – improving model performance

```{r}
# Increase the number of hidden nodes to five

concrete_model2 <- neuralnet(strength ~ cement + slag +
ash + water + superplastic +
coarseagg + fineagg + age,
data = concrete_train, hidden = 5)
plot(concrete_model2)
model_results2 <- compute(concrete_model2, concrete_test[1:8])
predicted_strength2 <- model_results2$net.result
cor(predicted_strength2, concrete_test$strength)
```

Problem 2

Build an R Notebook of the optical character recognition example in the textbook on pages 249 to 257. Show each step and add appropriate documentation.

Step 1 – exploring and preparing the data
```{r}
letters <- read.csv("letterdata.csv")
str(letters)
# create training and testing data frames
letters_train <- letters[1:16000, ]
letters_test <- letters[16001:20000, ]
```

Step 2 – training a model on the data

```{r}
letter_classifier <- ksvm(letter ~ ., data = letters_train,
kernel = "vanilladot")
letter_classifier

```

Step 3 – evaluating model performance

```{r}
# Make predictions
letter_predictions <- predict(letter_classifier, letters_test)
head(letter_predictions)
table(letter_predictions, letters_test$letter)

# The following command returns a vector of TRUE or FALSE values, indicating whether the model's predicted letter agrees with (that is, matches) the actual letter in the test dataset whether the model's predicted letter agrees with (that is, matches) the actual letter in the test dataset:
agreement <- letter_predictions == letters_test$letter
table(agreement)
prop.table(table(agreement))
```
Step 4 – improving model performance

```{r}
# Choose the Gaussian RBF kernel
letter_classifier_rbf <- ksvm(letter ~ ., data = letters_train,
kernel = "rbfdot")
# Make prediction
letter_predictions_rbf <- predict(letter_classifier_rbf,
letters_test)
# Compare the accuracy to our linear SVM
agreement_rbf <- letter_predictions_rbf == letters_test$letter
table(agreement_rbf)
prop.table(table(agreement_rbf))
```

Problem 3

Build an R Notebook of the grocery store transactions example in the textbook on pages 266 to 284. Show each step and add appropriate documentation.

```{r}
groceries <- read.transactions("groceries.csv", sep = ",")
summary(groceries)
inspect(groceries[1:5])
# View the support level for the first three items in the grocery data
itemFrequency(groceries[, 1:3])
```
Visualizing item support – item frequency plots

```{r}
# Visualize items appearing in a minimum proportion of transactions, with the support parameter using itenFrequencyPlot()
itemFrequencyPlot(groceries, support = 0.1)
# See the top 20 items with the topN parameter
itemFrequencyPlot(groceries, topN = 20)
```

Visualizing the transaction data – plotting the sparse matrix

```{r}
image(groceries[1:5])
# create random selection of 100 transactions 
image(sample(groceries, 100))
```

Step 3 – training a model on the data

```{r}
# Use the default settings of support = 0.1 and confidence = 0.8
apriori(groceries)
# Find a set of association rules using the Apriori algorithm
groceryrules <- apriori(groceries, parameter = list(support =
0.006, confidence = 0.25, minlen = 2))
groceryrules
summary(groceryrules)
inspect(groceryrules[1:3])
```

Step 5 – improving model performance

Sorting the set of association rules
```{r}
inspect(sort(groceryrules, by = "lift")[1:5])
```
Taking subsets of association rules
```{r}
# Find any rules with berries appearing in the rule
berryrules <- subset(groceryrules, items %in% "berries")
inspect(berryrules)
```
Saving association rules to a file or data frame
```{r}
# Save the rules to a CSV file
write(groceryrules, file = "groceryrules.csv",
sep = ",", quote = TRUE, row.names = FALSE)
# Convert the rules into an R data frame
groceryrules_df <- as(groceryrules, "data.frame")
str(groceryrules_df)
```

